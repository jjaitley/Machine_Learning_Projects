{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK\n",
    "\n",
    "A telecommunications company is concerned about the number of customers leaving them for other competitors. They need to understand who is leaving. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About dataset\n",
    "\n",
    "We will use a telecommunications data for predicting customer churn. This is a historical customer data where each row represents one customer. Typically it is less expensive to keep customers than acquire new ones, so the focus of this analysis is to predict the customers who will stay with the company.\n",
    "\n",
    "This data set provides info to help you predict behavior to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.\n",
    "\n",
    "The data set includes information about:\n",
    "\n",
    "**Customers who left within the last month** – the column is called Churn\n",
    "\n",
    "**Services that each customer has signed up for** – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n",
    "\n",
    "**Customer account information** – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n",
    "\n",
    "**Demographic info about customers** – gender, age range, and if they have partners and dependents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load Data From CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>equip</th>\n",
       "      <th>callcard</th>\n",
       "      <th>wireless</th>\n",
       "      <th>longmon</th>\n",
       "      <th>...</th>\n",
       "      <th>pager</th>\n",
       "      <th>internet</th>\n",
       "      <th>callwait</th>\n",
       "      <th>confer</th>\n",
       "      <th>ebill</th>\n",
       "      <th>loglong</th>\n",
       "      <th>logtoll</th>\n",
       "      <th>lninc</th>\n",
       "      <th>custcat</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.482</td>\n",
       "      <td>3.033</td>\n",
       "      <td>4.913</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.246</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.497</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.841</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.401</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.800</td>\n",
       "      <td>3.807</td>\n",
       "      <td>4.331</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.960</td>\n",
       "      <td>3.091</td>\n",
       "      <td>4.382</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>68.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.030</td>\n",
       "      <td>3.240</td>\n",
       "      <td>4.787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.110</td>\n",
       "      <td>3.157</td>\n",
       "      <td>3.611</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tenure   age  address  income   ed  employ  equip  callcard  wireless  \\\n",
       "0    11.0  33.0      7.0   136.0  5.0     5.0    0.0       1.0       1.0   \n",
       "1    33.0  33.0     12.0    33.0  2.0     0.0    0.0       0.0       0.0   \n",
       "2    23.0  30.0      9.0    30.0  1.0     2.0    0.0       0.0       0.0   \n",
       "3    38.0  35.0      5.0    76.0  2.0    10.0    1.0       1.0       1.0   \n",
       "4     7.0  35.0     14.0    80.0  2.0    15.0    0.0       1.0       0.0   \n",
       "5    68.0  52.0     17.0   120.0  1.0    24.0    0.0       1.0       0.0   \n",
       "6    42.0  40.0      7.0    37.0  2.0     8.0    1.0       1.0       1.0   \n",
       "\n",
       "   longmon  ...    pager  internet  callwait  confer  ebill  loglong  logtoll  \\\n",
       "0     4.40  ...      1.0       0.0       1.0     1.0    0.0    1.482    3.033   \n",
       "1     9.45  ...      0.0       0.0       0.0     0.0    0.0    2.246    3.240   \n",
       "2     6.30  ...      0.0       0.0       0.0     1.0    0.0    1.841    3.240   \n",
       "3     6.05  ...      1.0       1.0       1.0     1.0    1.0    1.800    3.807   \n",
       "4     7.10  ...      0.0       0.0       1.0     1.0    0.0    1.960    3.091   \n",
       "5    20.70  ...      0.0       0.0       0.0     0.0    0.0    3.030    3.240   \n",
       "6     8.25  ...      0.0       1.0       1.0     1.0    1.0    2.110    3.157   \n",
       "\n",
       "   lninc  custcat  churn  \n",
       "0  4.913      4.0    1.0  \n",
       "1  3.497      1.0    1.0  \n",
       "2  3.401      3.0    0.0  \n",
       "3  4.331      4.0    0.0  \n",
       "4  4.382      3.0    0.0  \n",
       "5  4.787      1.0    0.0  \n",
       "6  3.611      4.0    0.0  \n",
       "\n",
       "[7 rows x 28 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df = pd.read_csv(\"ChurnData.csv\")\n",
    "churn_df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing and selection\n",
    "\n",
    "Lets select some features for the modeling. Also we change the target data type to be integer, as it is a requirement by the skitlearn algorithm: And I will be using sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>equip</th>\n",
       "      <th>callcard</th>\n",
       "      <th>wireless</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>68.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tenure   age  address  income   ed  employ  equip  callcard  wireless  \\\n",
       "0    11.0  33.0      7.0   136.0  5.0     5.0    0.0       1.0       1.0   \n",
       "1    33.0  33.0     12.0    33.0  2.0     0.0    0.0       0.0       0.0   \n",
       "2    23.0  30.0      9.0    30.0  1.0     2.0    0.0       0.0       0.0   \n",
       "3    38.0  35.0      5.0    76.0  2.0    10.0    1.0       1.0       1.0   \n",
       "4     7.0  35.0     14.0    80.0  2.0    15.0    0.0       1.0       0.0   \n",
       "5    68.0  52.0     17.0   120.0  1.0    24.0    0.0       1.0       0.0   \n",
       "6    42.0  40.0      7.0    37.0  2.0     8.0    1.0       1.0       1.0   \n",
       "\n",
       "   churn  \n",
       "0      1  \n",
       "1      1  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      0  \n",
       "6      0  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip','callcard', 'wireless','churn']]\n",
    "\n",
    "churn_df['churn'] = churn_df['churn'].astype('int')\n",
    "\n",
    "churn_df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets define X, and y for our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.,  33.,   7., 136.,   5.,   5.,   0.],\n",
       "       [ 33.,  33.,  12.,  33.,   2.,   0.,   0.],\n",
       "       [ 23.,  30.,   9.,  30.,   1.,   2.,   0.],\n",
       "       [ 38.,  35.,   5.,  76.,   2.,  10.,   1.],\n",
       "       [  7.,  35.,  14.,  80.,   2.,  15.,   0.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])\n",
    "X[0:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.asarray(churn_df['churn'])\n",
    "y [0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now normalizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.14, -0.63, -0.46,  0.48,  1.7 , -0.58, -0.86],\n",
       "       [-0.12, -0.63,  0.03, -0.33, -0.64, -1.14, -0.86],\n",
       "       [-0.58, -0.86, -0.26, -0.35, -1.42, -0.92, -0.86]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "X[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (160, 7) (160,)\n",
      "Test set: (40, 7) (40,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling (Logistic Regression with Scikit-learn)\n",
    "\n",
    "The version of Logistic Regression in Scikit-learn, support regularization. Regularization is a technique used to solve the overfitting problem in machine learning models. **C parameter indicates inverse of regularization strength which must be a positive float**. Smaller values specify stronger regularization. Now lets fit our model with train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "clf_Lo_Reg = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\n",
    "clf_Lo_Reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict using the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf_Lo_Reg.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### predict_proba \n",
    "returns estimates for all classes, ordered by the label of classes. So, the first column is the probability of class 1, P(Y=1|X), and second column is probability of class 0, P(Y=0|X):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54, 0.46],\n",
       "       [0.61, 0.39],\n",
       "       [0.56, 0.44],\n",
       "       [0.63, 0.37],\n",
       "       [0.56, 0.44],\n",
       "       [0.55, 0.45],\n",
       "       [0.52, 0.48],\n",
       "       [0.61, 0.39],\n",
       "       [0.41, 0.59],\n",
       "       [0.63, 0.37],\n",
       "       [0.58, 0.42],\n",
       "       [0.63, 0.37],\n",
       "       [0.48, 0.52],\n",
       "       [0.43, 0.57],\n",
       "       [0.66, 0.34],\n",
       "       [0.55, 0.45],\n",
       "       [0.52, 0.48],\n",
       "       [0.49, 0.51],\n",
       "       [0.49, 0.51],\n",
       "       [0.52, 0.48],\n",
       "       [0.62, 0.38],\n",
       "       [0.53, 0.47],\n",
       "       [0.64, 0.36],\n",
       "       [0.52, 0.48],\n",
       "       [0.51, 0.49],\n",
       "       [0.71, 0.29],\n",
       "       [0.55, 0.45],\n",
       "       [0.52, 0.48],\n",
       "       [0.52, 0.48],\n",
       "       [0.71, 0.29],\n",
       "       [0.68, 0.32],\n",
       "       [0.51, 0.49],\n",
       "       [0.42, 0.58],\n",
       "       [0.71, 0.29],\n",
       "       [0.6 , 0.4 ],\n",
       "       [0.64, 0.36],\n",
       "       [0.4 , 0.6 ],\n",
       "       [0.52, 0.48],\n",
       "       [0.66, 0.34],\n",
       "       [0.51, 0.49]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob = clf_Lo_Reg.predict_proba(X_test)\n",
    "pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "***jaccard index***\n",
    "\n",
    "Lets try jaccard index for accuracy evaluation. we can define jaccard as the size of the intersection divided by the size of the union of two label sets. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "jaccard_similarity_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buliding a confusion matrix\n",
    "\n",
    "Another way of looking at accuracy of classifier is to look at **confusion matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  9]\n",
      " [ 1 24]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "print(confusion_matrix(y_test, pred, labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 6  9]\n",
      " [ 1 24]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, pred, labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['churn=1','churn=0'],normalize= False,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row is for customers whose actual churn value in test set is 1. As we can calculate, out of 40 customers, the churn value of 15 of them is 1. And out of these 15, the classifier correctly predicted 6 of them as 1, and 9 of them as 0.\n",
    "\n",
    "It means, for 6 customers, the actual churn value were 1 in test set, and classifier also correctly predicted those as 1. However, while the actual label of 9 customers were 1, the classifier predicted those as 0, which is not very good. We can consider it as error of the model for first row.\n",
    "\n",
    "What about the customers with churn value 0? Lets look at the second row. It looks like there were 25 customers whom their churn value were 0.\n",
    "\n",
    "The classifier correctly predicted 24 of them as 0, and one of them wrongly as 1. So, it has done a good job in predicting the customers with churn value 0. A good thing about confusion matrix is that shows the model’s ability to correctly predict or separate the classes. In specific case of binary classifier, such as this example, we can interpret these numbers as the count of true positives, false positives, true negatives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.96      0.83        25\n",
      "          1       0.86      0.40      0.55        15\n",
      "\n",
      "avg / total       0.78      0.75      0.72        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpler method of making confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb6c9a20>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "skplt.metrics.plot_confusion_matrix(y_test, pred ,labels=[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log loss\n",
    "\n",
    "Now, lets try log loss for evaluation. In logistic regression, the output can be the probability of customer churn is yes (or equals to 1). This probability is a value between 0 and 1. Log loss( Logarithmic loss) measures the performance of a classifier where the predicted output is a probability value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6017092478101186"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, pred_prob) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Fitting using Gradient descent\n",
    "\n",
    "We should find the best parameters for our model by minimizing the cost function of our model. \n",
    "\n",
    "To minimize the cost function we use below -- **Using an optimization approach. There are different optimization approaches, but we use one of famous and effective approaches here, gradient descent.**\n",
    "\n",
    "**Generally, gradient descent is an iterative approach to finding the minimum of a function. Specifically, in our case, gradient descent is a technique to use derivative of a cost function to change the parameter values, to minimize the cost/error.**\n",
    "\n",
    "Sigmoid function’s output is always between 0 and 1, which make it proper to interpret the results as probabilities. It is obvious that, when the outcome of sigma function get closer to 1, the probability of y=1, given x, goes up, and in contrast, when the sigmoid value is closer to zero, the probability of y=1, given x, is very small. Here we define a function to compute the sigmoid of an input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the sigmoid function\n",
    "def sigmoid(z):\n",
    "    s =  1.0 / (1.0 + np.exp(- z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to compare the output of our model with the actual label of the customer. Then, record the difference as our model’s error for each customer. The total error (for all customers) is cost of your model, and is calculated by model’s cost function. The cost function, by the way, basically represents how to calculate the error of the model, which is the difference between actual and the model’s predicted values. However, Logistic regression, uses a specific cost function which penalizes situations in which the class is 0 and the model output is 1, and vice versa. It uses log-likelihood to form the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes cost given predicted and actual values\n",
    "def cost_computation(theta, X, y):\n",
    "    hx = sigmoid(np.dot(X, theta)) # predicted probability of label 1\n",
    "    cost = (-y)* np.log(hx) - (1-y)*np.log(1-hx) # log-likelihood vector\n",
    "    J = cost.mean()\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Follow the curve by calculating the gradients or the first derivatives of the cost function with respect to each theta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_computation(theta, X, y):\n",
    "    hx = sigmoid(np.dot(X, theta))\n",
    "    error = hx - y # difference between label and prediction\n",
    "    grad = np.dot(error, X) / y.size # gradient vector\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This function predicts whether the label is 0 or 1 using learned logistic regression parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(theta, X):\n",
    "    m, n = X.shape\n",
    "    p = np.zeros(shape=(m, 1))\n",
    "\n",
    "    h = sigmoid(X.dot(theta.T))\n",
    "\n",
    "    for it in range(0, h.shape[0]):\n",
    "        if h[it] > 0.5:\n",
    "            p[it, 0] = 1\n",
    "        else:\n",
    "            p[it, 0] = 0\n",
    "\n",
    "    return p.reshape(m,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, theta, alpha, iterations):\n",
    "#gradient descent algorithm to find optimal theta values\n",
    "    theta_n = theta.size  \n",
    "    for i in range(iterations):\n",
    "        h = sigmoid(np.dot(x, theta))\n",
    "        gradient_val = grad_computation(theta, x, y)\n",
    "        theta= theta - alpha * gradient_val\n",
    "        print('>iteration=%d, lrate=%.3f, cost=%.3f' % (i, alpha, cost_computation(theta, x, y)))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**we add 1 to as first value of each parameter vector, to play intrecept of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  1.09,  0.14, -0.75,  0.2 , -0.64,  1.32, -0.86],\n",
       "       [ 1.  ,  0.86,  0.06, -1.15, -0.16,  0.92, -0.92,  1.16],\n",
       "       [ 1.  ,  0.95,  0.91,  1.12,  0.19,  1.7 ,  0.42,  1.16],\n",
       "       [ 1.  ,  1.55,  0.06,  1.12, -0.44,  0.14, -1.14,  1.16],\n",
       "       [ 1.  ,  1.41,  0.68, -0.95,  2.01,  1.7 ,  1.54, -0.86]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1 = np.append( np.ones((X_train.shape[0], 1)), X_train, axis=1)\n",
    "X_test_1 = np.append( np.ones((X_test.shape[0], 1)), X_test, axis=1)\n",
    "X_train_1[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">iteration=0, lrate=0.100, cost=0.800\n",
      ">iteration=1, lrate=0.100, cost=0.770\n",
      ">iteration=2, lrate=0.100, cost=0.745\n",
      ">iteration=3, lrate=0.100, cost=0.721\n",
      ">iteration=4, lrate=0.100, cost=0.701\n",
      ">iteration=5, lrate=0.100, cost=0.683\n",
      ">iteration=6, lrate=0.100, cost=0.666\n",
      ">iteration=7, lrate=0.100, cost=0.652\n",
      ">iteration=8, lrate=0.100, cost=0.639\n",
      ">iteration=9, lrate=0.100, cost=0.627\n",
      ">iteration=10, lrate=0.100, cost=0.617\n",
      ">iteration=11, lrate=0.100, cost=0.607\n",
      ">iteration=12, lrate=0.100, cost=0.599\n",
      ">iteration=13, lrate=0.100, cost=0.591\n",
      ">iteration=14, lrate=0.100, cost=0.583\n",
      ">iteration=15, lrate=0.100, cost=0.577\n",
      ">iteration=16, lrate=0.100, cost=0.571\n",
      ">iteration=17, lrate=0.100, cost=0.565\n",
      ">iteration=18, lrate=0.100, cost=0.560\n",
      ">iteration=19, lrate=0.100, cost=0.555\n",
      ">iteration=20, lrate=0.100, cost=0.551\n",
      ">iteration=21, lrate=0.100, cost=0.546\n",
      ">iteration=22, lrate=0.100, cost=0.542\n",
      ">iteration=23, lrate=0.100, cost=0.539\n",
      ">iteration=24, lrate=0.100, cost=0.535\n",
      ">iteration=25, lrate=0.100, cost=0.532\n",
      ">iteration=26, lrate=0.100, cost=0.529\n",
      ">iteration=27, lrate=0.100, cost=0.526\n",
      ">iteration=28, lrate=0.100, cost=0.523\n",
      ">iteration=29, lrate=0.100, cost=0.520\n",
      ">iteration=30, lrate=0.100, cost=0.517\n",
      ">iteration=31, lrate=0.100, cost=0.515\n",
      ">iteration=32, lrate=0.100, cost=0.513\n",
      ">iteration=33, lrate=0.100, cost=0.510\n",
      ">iteration=34, lrate=0.100, cost=0.508\n",
      ">iteration=35, lrate=0.100, cost=0.506\n",
      ">iteration=36, lrate=0.100, cost=0.504\n",
      ">iteration=37, lrate=0.100, cost=0.502\n",
      ">iteration=38, lrate=0.100, cost=0.501\n",
      ">iteration=39, lrate=0.100, cost=0.499\n",
      ">iteration=40, lrate=0.100, cost=0.497\n",
      ">iteration=41, lrate=0.100, cost=0.496\n",
      ">iteration=42, lrate=0.100, cost=0.494\n",
      ">iteration=43, lrate=0.100, cost=0.493\n",
      ">iteration=44, lrate=0.100, cost=0.491\n",
      ">iteration=45, lrate=0.100, cost=0.490\n",
      ">iteration=46, lrate=0.100, cost=0.488\n",
      ">iteration=47, lrate=0.100, cost=0.487\n",
      ">iteration=48, lrate=0.100, cost=0.486\n",
      ">iteration=49, lrate=0.100, cost=0.485\n",
      ">iteration=50, lrate=0.100, cost=0.484\n",
      ">iteration=51, lrate=0.100, cost=0.482\n",
      ">iteration=52, lrate=0.100, cost=0.481\n",
      ">iteration=53, lrate=0.100, cost=0.480\n",
      ">iteration=54, lrate=0.100, cost=0.479\n",
      ">iteration=55, lrate=0.100, cost=0.478\n",
      ">iteration=56, lrate=0.100, cost=0.477\n",
      ">iteration=57, lrate=0.100, cost=0.476\n",
      ">iteration=58, lrate=0.100, cost=0.476\n",
      ">iteration=59, lrate=0.100, cost=0.475\n",
      ">iteration=60, lrate=0.100, cost=0.474\n",
      ">iteration=61, lrate=0.100, cost=0.473\n",
      ">iteration=62, lrate=0.100, cost=0.472\n",
      ">iteration=63, lrate=0.100, cost=0.471\n",
      ">iteration=64, lrate=0.100, cost=0.471\n",
      ">iteration=65, lrate=0.100, cost=0.470\n",
      ">iteration=66, lrate=0.100, cost=0.469\n",
      ">iteration=67, lrate=0.100, cost=0.469\n",
      ">iteration=68, lrate=0.100, cost=0.468\n",
      ">iteration=69, lrate=0.100, cost=0.467\n",
      ">iteration=70, lrate=0.100, cost=0.467\n",
      ">iteration=71, lrate=0.100, cost=0.466\n",
      ">iteration=72, lrate=0.100, cost=0.465\n",
      ">iteration=73, lrate=0.100, cost=0.465\n",
      ">iteration=74, lrate=0.100, cost=0.464\n",
      ">iteration=75, lrate=0.100, cost=0.464\n",
      ">iteration=76, lrate=0.100, cost=0.463\n",
      ">iteration=77, lrate=0.100, cost=0.463\n",
      ">iteration=78, lrate=0.100, cost=0.462\n",
      ">iteration=79, lrate=0.100, cost=0.462\n",
      ">iteration=80, lrate=0.100, cost=0.461\n",
      ">iteration=81, lrate=0.100, cost=0.461\n",
      ">iteration=82, lrate=0.100, cost=0.460\n",
      ">iteration=83, lrate=0.100, cost=0.460\n",
      ">iteration=84, lrate=0.100, cost=0.459\n",
      ">iteration=85, lrate=0.100, cost=0.459\n",
      ">iteration=86, lrate=0.100, cost=0.459\n",
      ">iteration=87, lrate=0.100, cost=0.458\n",
      ">iteration=88, lrate=0.100, cost=0.458\n",
      ">iteration=89, lrate=0.100, cost=0.457\n",
      ">iteration=90, lrate=0.100, cost=0.457\n",
      ">iteration=91, lrate=0.100, cost=0.457\n",
      ">iteration=92, lrate=0.100, cost=0.456\n",
      ">iteration=93, lrate=0.100, cost=0.456\n",
      ">iteration=94, lrate=0.100, cost=0.456\n",
      ">iteration=95, lrate=0.100, cost=0.455\n",
      ">iteration=96, lrate=0.100, cost=0.455\n",
      ">iteration=97, lrate=0.100, cost=0.455\n",
      ">iteration=98, lrate=0.100, cost=0.454\n",
      ">iteration=99, lrate=0.100, cost=0.454\n"
     ]
    }
   ],
   "source": [
    "# prefix an extra column of ones to the feature matrix (for intercept term)\n",
    "theta_0 = 0.1* np.random.randn(X_train_1.shape[1])\n",
    "theta = gradient_descent(X_train_1, y_train, theta_0, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 78.125000\n"
     ]
    }
   ],
   "source": [
    "pred = predict_class(np.array(theta), X_train_1)\n",
    "#Compute accuracy on our training set\n",
    "print ('Train Accuracy: %f' % ((y_train[(pred == y_train)].size / float(y_train.size)) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 72.500000\n"
     ]
    }
   ],
   "source": [
    "pred = predict_class(np.array(theta), X_test_1)\n",
    "#Compute accuracy on our training set\n",
    "print ('Test Accuracy: %f' % ((y_test[(pred == y_test)].size / float(y_test.size)) * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Fitting using bfgs algorithm\n",
    "\n",
    "Also, we can use **fmin_bfgs** to minimize the cost function. **fmin_bfgs** is a scipy built-in function which finds the best parameters theta for the logistic regression cost function given a fixed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.437695\n",
      "         Iterations: 27\n",
      "         Function evaluations: 28\n",
      "         Gradient evaluations: 28\n"
     ]
    }
   ],
   "source": [
    "theta = opt.fmin_bfgs(cost_computation, theta_0, fprime=grad_computation, args=(X_train_1, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Fitting using Stochastic Gradient Descent\n",
    "\n",
    "What if we estimate gradient with just one sample?\n",
    "\n",
    "Gradient Descent is the process of minimizing our cost function by following the gradients of the cost function.\n",
    "\n",
    "'Stochastic Gradient Descent' is an optimization algorithem where we update the coefficients of the model in every iteration to minimize the error of a model on the training data. The way this algorithm works is that each training instance is shown to the model one at a time. The model makes a prediction for a training instance, the error is calculated and the model is updated in order to reduce the error for the next prediction.\n",
    "\n",
    "**Stochastic Gradient Descent**\n",
    "\n",
    "\n",
    "In this function we calculate the error for each prediction and update the theta accordingly. The error is calculated as the difference between the predication value and the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate h_theta -- Predictionof a row\n",
    "def predict_row(row, theta):\n",
    "    hx = sigmoid(np.dot(row, theta))\n",
    "    return hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate theta using stochastic gradient descent\n",
    "def theta_sgd(X_train, y_train, alpha, n_epoch):\n",
    "    theta = [0.0 for i in range(len(X_train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for x,y in zip(X_train_1,y_train):\n",
    "            ht = predict_row(x, theta)\n",
    "            error =  ht - y\n",
    "            theta[0] = theta[0] - alpha * error \n",
    "            for i in range(len(theta)-1):\n",
    "                theta[i + 1] = theta[i + 1] - alpha * error  * x[i+1]\n",
    "        sum_error += error**2\n",
    "        # cost computation\n",
    "        cost = cost_computation(theta, X_train, y_train)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f, cost=%.3f' % (epoch, alpha, sum_error,cost))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.001, error=0.263, cost=0.669\n",
      ">epoch=1, lrate=0.001, error=0.274, cost=0.649\n",
      ">epoch=2, lrate=0.001, error=0.283, cost=0.632\n",
      ">epoch=3, lrate=0.001, error=0.292, cost=0.617\n",
      ">epoch=4, lrate=0.001, error=0.299, cost=0.604\n",
      ">epoch=5, lrate=0.001, error=0.305, cost=0.593\n",
      ">epoch=6, lrate=0.001, error=0.310, cost=0.583\n",
      ">epoch=7, lrate=0.001, error=0.314, cost=0.574\n",
      ">epoch=8, lrate=0.001, error=0.318, cost=0.566\n",
      ">epoch=9, lrate=0.001, error=0.321, cost=0.558\n",
      ">epoch=10, lrate=0.001, error=0.324, cost=0.552\n",
      ">epoch=11, lrate=0.001, error=0.326, cost=0.546\n",
      ">epoch=12, lrate=0.001, error=0.328, cost=0.540\n",
      ">epoch=13, lrate=0.001, error=0.329, cost=0.535\n",
      ">epoch=14, lrate=0.001, error=0.331, cost=0.530\n",
      ">epoch=15, lrate=0.001, error=0.332, cost=0.526\n",
      ">epoch=16, lrate=0.001, error=0.332, cost=0.522\n",
      ">epoch=17, lrate=0.001, error=0.333, cost=0.518\n",
      ">epoch=18, lrate=0.001, error=0.333, cost=0.514\n",
      ">epoch=19, lrate=0.001, error=0.333, cost=0.511\n",
      ">epoch=20, lrate=0.001, error=0.333, cost=0.508\n",
      ">epoch=21, lrate=0.001, error=0.333, cost=0.505\n",
      ">epoch=22, lrate=0.001, error=0.333, cost=0.502\n",
      ">epoch=23, lrate=0.001, error=0.333, cost=0.499\n",
      ">epoch=24, lrate=0.001, error=0.333, cost=0.497\n",
      ">epoch=25, lrate=0.001, error=0.333, cost=0.494\n",
      ">epoch=26, lrate=0.001, error=0.332, cost=0.492\n",
      ">epoch=27, lrate=0.001, error=0.332, cost=0.490\n",
      ">epoch=28, lrate=0.001, error=0.331, cost=0.488\n",
      ">epoch=29, lrate=0.001, error=0.331, cost=0.486\n",
      ">epoch=30, lrate=0.001, error=0.330, cost=0.484\n",
      ">epoch=31, lrate=0.001, error=0.330, cost=0.482\n",
      ">epoch=32, lrate=0.001, error=0.329, cost=0.481\n",
      ">epoch=33, lrate=0.001, error=0.329, cost=0.479\n",
      ">epoch=34, lrate=0.001, error=0.328, cost=0.478\n",
      ">epoch=35, lrate=0.001, error=0.328, cost=0.476\n",
      ">epoch=36, lrate=0.001, error=0.327, cost=0.475\n",
      ">epoch=37, lrate=0.001, error=0.326, cost=0.474\n",
      ">epoch=38, lrate=0.001, error=0.326, cost=0.472\n",
      ">epoch=39, lrate=0.001, error=0.325, cost=0.471\n",
      ">epoch=40, lrate=0.001, error=0.325, cost=0.470\n",
      ">epoch=41, lrate=0.001, error=0.324, cost=0.469\n",
      ">epoch=42, lrate=0.001, error=0.324, cost=0.468\n",
      ">epoch=43, lrate=0.001, error=0.323, cost=0.467\n",
      ">epoch=44, lrate=0.001, error=0.323, cost=0.466\n",
      ">epoch=45, lrate=0.001, error=0.322, cost=0.465\n",
      ">epoch=46, lrate=0.001, error=0.322, cost=0.464\n",
      ">epoch=47, lrate=0.001, error=0.321, cost=0.463\n",
      ">epoch=48, lrate=0.001, error=0.321, cost=0.463\n",
      ">epoch=49, lrate=0.001, error=0.320, cost=0.462\n",
      ">epoch=50, lrate=0.001, error=0.320, cost=0.461\n",
      ">epoch=51, lrate=0.001, error=0.319, cost=0.460\n",
      ">epoch=52, lrate=0.001, error=0.319, cost=0.460\n",
      ">epoch=53, lrate=0.001, error=0.319, cost=0.459\n",
      ">epoch=54, lrate=0.001, error=0.318, cost=0.458\n",
      ">epoch=55, lrate=0.001, error=0.318, cost=0.458\n",
      ">epoch=56, lrate=0.001, error=0.317, cost=0.457\n",
      ">epoch=57, lrate=0.001, error=0.317, cost=0.457\n",
      ">epoch=58, lrate=0.001, error=0.317, cost=0.456\n",
      ">epoch=59, lrate=0.001, error=0.316, cost=0.455\n",
      ">epoch=60, lrate=0.001, error=0.316, cost=0.455\n",
      ">epoch=61, lrate=0.001, error=0.316, cost=0.454\n",
      ">epoch=62, lrate=0.001, error=0.315, cost=0.454\n",
      ">epoch=63, lrate=0.001, error=0.315, cost=0.453\n",
      ">epoch=64, lrate=0.001, error=0.315, cost=0.453\n",
      ">epoch=65, lrate=0.001, error=0.314, cost=0.453\n",
      ">epoch=66, lrate=0.001, error=0.314, cost=0.452\n",
      ">epoch=67, lrate=0.001, error=0.314, cost=0.452\n",
      ">epoch=68, lrate=0.001, error=0.313, cost=0.451\n",
      ">epoch=69, lrate=0.001, error=0.313, cost=0.451\n",
      ">epoch=70, lrate=0.001, error=0.313, cost=0.451\n",
      ">epoch=71, lrate=0.001, error=0.313, cost=0.450\n",
      ">epoch=72, lrate=0.001, error=0.312, cost=0.450\n",
      ">epoch=73, lrate=0.001, error=0.312, cost=0.450\n",
      ">epoch=74, lrate=0.001, error=0.312, cost=0.449\n",
      ">epoch=75, lrate=0.001, error=0.312, cost=0.449\n",
      ">epoch=76, lrate=0.001, error=0.312, cost=0.449\n",
      ">epoch=77, lrate=0.001, error=0.311, cost=0.448\n",
      ">epoch=78, lrate=0.001, error=0.311, cost=0.448\n",
      ">epoch=79, lrate=0.001, error=0.311, cost=0.448\n",
      ">epoch=80, lrate=0.001, error=0.311, cost=0.448\n",
      ">epoch=81, lrate=0.001, error=0.311, cost=0.447\n",
      ">epoch=82, lrate=0.001, error=0.310, cost=0.447\n",
      ">epoch=83, lrate=0.001, error=0.310, cost=0.447\n",
      ">epoch=84, lrate=0.001, error=0.310, cost=0.447\n",
      ">epoch=85, lrate=0.001, error=0.310, cost=0.446\n",
      ">epoch=86, lrate=0.001, error=0.310, cost=0.446\n",
      ">epoch=87, lrate=0.001, error=0.310, cost=0.446\n",
      ">epoch=88, lrate=0.001, error=0.309, cost=0.446\n",
      ">epoch=89, lrate=0.001, error=0.309, cost=0.446\n",
      ">epoch=90, lrate=0.001, error=0.309, cost=0.445\n",
      ">epoch=91, lrate=0.001, error=0.309, cost=0.445\n",
      ">epoch=92, lrate=0.001, error=0.309, cost=0.445\n",
      ">epoch=93, lrate=0.001, error=0.309, cost=0.445\n",
      ">epoch=94, lrate=0.001, error=0.309, cost=0.445\n",
      ">epoch=95, lrate=0.001, error=0.308, cost=0.444\n",
      ">epoch=96, lrate=0.001, error=0.308, cost=0.444\n",
      ">epoch=97, lrate=0.001, error=0.308, cost=0.444\n",
      ">epoch=98, lrate=0.001, error=0.308, cost=0.444\n",
      ">epoch=99, lrate=0.001, error=0.308, cost=0.444\n"
     ]
    }
   ],
   "source": [
    "theta = theta_sgd(X_train_1,y_train, 0.001, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 77.500000\n"
     ]
    }
   ],
   "source": [
    "pred = predict_class(np.array(theta), X_train_1)\n",
    "#Compute accuracy on our training set\n",
    "print ('Train Accuracy: %f' % ((y_train[(pred == y_train)].size / float(y_train.size)) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 72.500000\n"
     ]
    }
   ],
   "source": [
    "pred = predict_class(np.array(theta), X_test_1)\n",
    "#Compute accuracy on our training set\n",
    "print ('Test Accuracy: %f' % ((y_test[(pred == y_test)].size / float(y_test.size)) * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "There is a parameter, regularization, that is used for preventing over fitting. We can find the best regularization value using accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72, 0.72, 0.72, 0.72])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Regularization_Inv=[0.00001,0.1,1,100]\n",
    "Length=len(Regularization_Inv)\n",
    "mean_acc=np.zeros((Length))\n",
    "std_acc=np.zeros((Length))\n",
    "ConfustionMx=[];\n",
    "\n",
    "\n",
    "for Reg,n in zip(Regularization_Inv,range(0,Length)):\n",
    "    \n",
    "    LR = LogisticRegression(C=Reg).fit(X_train,y_train)\n",
    "    yhat=LR.predict(X_test)\n",
    "    mean_acc[n]=np.mean(yhat==y_test);\n",
    "    \n",
    "    \n",
    "    std_acc[n]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n",
    "    ConfustionMx.append(confusion_matrix(yhat,y_test,labels=[1,0]))\n",
    "mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy for Logistic regression is 0.725\n"
     ]
    }
   ],
   "source": [
    "print( \"The best accuracy for Logistic regression is\", mean_acc.max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
